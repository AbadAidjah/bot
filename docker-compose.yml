services:
  api:
    build: ./api
    container_name: llm-api
    ports:
      - "8000:8000"
    environment:
      - API_TOKEN=SUPER_SECRET_TOKEN_123
    # Model downloads to cache, persist it for faster restarts
    volumes:
      - huggingface-cache:/root/.cache/huggingface

volumes:
  huggingface-cache:
